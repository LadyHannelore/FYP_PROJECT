{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install opencv-python matplotlib pandas ffmpeg-python cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "platform.architecture()\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\UMER\\AppData\\Local\\Temp\\ipykernel_12848\\528363516.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpu_available = tf.test.is_gpu_available()\n",
    "is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)\n",
    "is_cuda_gpu_min_3 = tf.test.is_gpu_available(True, (3,0))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(gpu_available)\n",
    "print(is_cuda_gpu_available)\n",
    "print(is_cuda_gpu_min_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Tester code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100, loss: 0.435450, accuracy: 0.864258\n",
      "step: 200, loss: 0.302959, accuracy: 0.923828\n",
      "step: 300, loss: 0.261871, accuracy: 0.922852\n",
      "step: 400, loss: 0.314974, accuracy: 0.911133\n",
      "step: 500, loss: 0.237219, accuracy: 0.934570\n",
      "Test Accuracy: 0.935500\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# MNIST dataset parameters.\n",
    "num_classes = 10 # total classes (0-9 digits).\n",
    "num_features = 784 # data features (img shape: 28*28).\n",
    "\n",
    "# Training parameters.\n",
    "learning_rate = 0.1\n",
    "training_steps = 500\n",
    "batch_size = 1024\n",
    "display_step = 100\n",
    "\n",
    "# Network parameters.\n",
    "n_hidden_1 = 128 # 1st layer number of neurons.\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons.\n",
    "\n",
    "# Prepare MNIST data.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Flatten images to 1-D vector of 784 features (28*28).\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "\n",
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n",
    "\n",
    "# Create TF Model.\n",
    "class NeuralNet(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # First fully-connected hidden layer.\n",
    "        self.fc1 = layers.Dense(n_hidden_1, activation=tf.nn.relu)\n",
    "        # First fully-connected hidden layer.\n",
    "        self.fc2 = layers.Dense(n_hidden_2, activation=tf.nn.relu)\n",
    "        # Second fully-connecter hidden layer.\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, is_training=False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build neural network model.\n",
    "neural_net = NeuralNet()\n",
    "\n",
    "# Cross-Entropy Loss.\n",
    "# Note that this will apply 'softmax' to the logits.\n",
    "def cross_entropy_loss(x, y):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        pred = neural_net(x, is_training=True)\n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(pred, y)\n",
    "        \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = neural_net.trainable_variables\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x, is_training=True)\n",
    "        loss = cross_entropy_loss(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n",
    "\n",
    "# Test model on validation set.\n",
    "pred = neural_net(x_test, is_training=False)\n",
    "print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import cv2\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract frames (done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import cv2\n",
    "def framer(vid_f,pathout,file):\n",
    "  count = 0\n",
    "  vidcap = cv2.VideoCapture(vid_f)\n",
    "  success,image = vidcap.read()\n",
    "  while success:\n",
    "    cv2.imwrite(f\"{pathout}/frame{file}_{count}.jpeg\", image) # save frame as JPEG file \n",
    "    count+=1          \n",
    "    success,image = vidcap.read()\n",
    "    print('Read a new frame: ', success)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Frames (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "#dataset_path = os.listdir('CricShot10 dataset\\\\sweep')\n",
    "#for dataset in dataset_path:\n",
    "count = 0\n",
    "dataset=\"sweep\"    \n",
    "os.makedirs(\"CricShot10 dataset/\" + dataset + '_frames/')\n",
    "for f in dataset_path:\n",
    "    count += 1\n",
    "    framer(\"CricShot10 dataset/\" + dataset + '/' + f, \"CricShot10 dataset/\" + dataset + '_frames', count)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGGModel(input_tensor = None, classes = 10): \n",
    "    \n",
    "\n",
    "\tinput_layer = tf.keras.layers.Input([350,330,3])\n",
    "\t#block1 \n",
    "\tconv1_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=[3,3],strides=[1,1], \n",
    "\t\tpadding='same',activation='relu', name='conv1_1')(input_layer)\n",
    "\n",
    "\tconv1_2 = tf.keras.layers.Conv2D(filters= 64, kernel_size=[3,3], strides= [1,1],\n",
    "\t\tpadding='same',activation='relu', name='conv1_2')(conv1_1)\n",
    "\n",
    "\tpool1_1 = tf.nn.max_pool(conv1_2, ksize = [1,2,2,1],strides=[1,2,2,1],\n",
    "\t\tpadding='SAME', name='pool1_1')\n",
    "\n",
    "\n",
    "\t#block 2\n",
    "\tconv2_1 = tf.keras.layers.Conv2D(filters= 128, kernel_size=[3,3],strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv2_1')(pool1_1)\n",
    "\n",
    "\n",
    "\tconv2_2 = tf.keras.layers.Conv2D(filters = 128, kernel_size=[3,3], strides = [1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv2_2')(conv2_1)\n",
    "\n",
    "\n",
    "\tpool2_1 = tf.nn.max_pool(conv2_2, ksize = [1,2,2,1], strides= [1,2,2,1],\n",
    "\t\tpadding='SAME', name='pool2_1')\n",
    "\n",
    "\n",
    "\n",
    "\t#block3 \n",
    "\n",
    "\tconv3_1 = tf.keras.layers.Conv2D(filters= 256, kernel_size=[3,3], strides = [1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv3_1')(pool2_1)\n",
    "\n",
    "\tconv3_2 = tf.keras.layers.Conv2D(filters = 256, kernel_size=[3,3], strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv3_2')(conv3_1)\n",
    "\n",
    "\tconv3_3 = tf.keras.layers.Conv2D(filters = 256, kernel_size= [3,3], strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv3_3')(conv3_2)\n",
    "\n",
    "\tpool3_1 = tf.nn.max_pool(conv3_3, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
    "\t\tpadding=\"SAME\",name='pool3_1')\n",
    "\n",
    "\t#block4 \n",
    "\n",
    "\tconv4_1 = tf.keras.layers.Conv2D(filters = 512, kernel_size= [3,3], strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv4_1')(pool3_1)\n",
    "\n",
    "\n",
    "\tconv4_2 = tf.keras.layers.Conv2D(filters = 512, kernel_size= [3,3], strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv4_2')(conv4_1)\n",
    "\n",
    "\n",
    "\tconv4_3 = tf.keras.layers.Conv2D(filters = 512, kernel_size= [3,3], strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv4_3')(conv4_2)\n",
    "\t\n",
    "\n",
    "\tpool4_1 = tf.nn.max_pool(conv4_3, ksize=[1,2,2,1],strides=[1,2,2,1],\n",
    "\t\tpadding='SAME', name='pool4_1')\n",
    "\n",
    "\t#block5\n",
    "\n",
    "\n",
    "\tconv5_1 = tf.keras.layers.Conv2D(filters = 512, kernel_size= [3,3], strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv5_1')(pool4_1)\n",
    "\n",
    "\tconv5_2 = tf.keras.layers.Conv2D(filters = 512, kernel_size= [3,3], strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv5_2')(conv5_1)\n",
    "\n",
    "\tconv5_3 = tf.keras.layers.Conv2D(filters = 512, kernel_size= [3,3], strides=[1,1],\n",
    "\t\tpadding='same', activation='relu', name='conv5_3')(conv5_2)\n",
    "\n",
    "\tpool5_1 = tf.nn.max_pool(conv5_3, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
    "\t\tpadding='SAME', name='pool5_1')\n",
    "\n",
    "\n",
    "\n",
    "\tflatten  = tf.keras.layers.Flatten()(pool5_1)\n",
    "\n",
    "\tfc6 = tf.keras.layers.Dense(units=4096, name='fc6', activation='relu')(flatten)\n",
    "\tfc7 = tf.keras.layers.Dense(units=4096, name='fc7', activation='relu')(fc6)\n",
    "\tfc8 = tf.keras.layers.Dense(units=1000, name='fc8',activation=None)(fc7)\n",
    "\n",
    "\n",
    "\tprob = tf.nn.softmax(fc8)\n",
    "\n",
    "\n",
    "\n",
    "\tmodel = tf.keras.Model(input_layer, prob)\n",
    " \n",
    "\treturn model\n",
    "\t#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os,random\n",
    "path = r'dataset frames\\\\sweep_frames'# You can provide the path here\n",
    "n = 2575 # Number of random images to be removed to reach 8,958\n",
    "img_names = os.listdir(path)  # Get image names in folder\n",
    "img_names = random.sample(img_names, n)  # Pick 2500 random images\n",
    "for image in img_names:  # Go over each image name to be deleted\n",
    "    f = os.path.join(path, image)  # Create valid path to image\n",
    "    os.remove(f)  # Remove the image'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import cv2 \n",
    "import pickle \n",
    "import tensorflow as tf\n",
    "\n",
    "data_dir = 'dataset frames'\n",
    "\n",
    "categories = ['cover_frames', 'defense_frames' , 'flick_frames', 'hook_frames', 'late_cut_frames', 'lofted_frames', 'pull_frames', 'square_cut_frames', 'straight_frames', 'sweep_frames']\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for category in categories:\n",
    "\tpath = os.path.join(data_dir, category)\n",
    "\tlabel = categories.index(category)\n",
    "\n",
    "\tfor img_name in os.listdir(path):\n",
    "\t\timage_path = os.path.join(path, img_name)\n",
    "\t\t#image = cv2.imread(image_path)\n",
    "\t\t\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\t#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\t\t\t#image = cv2.resize(image, (240,240))\n",
    "\t\t\t#image = np.array(image)\n",
    "\n",
    "\t\t\tdata.append([image_path, label])\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass\n",
    "\n",
    "\tprint(len(data))\n",
    "\tpik = open('shots.pickle','wb')\n",
    "\tpickle.dump(data, pik)\n",
    "\tpik.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pick  = open('shots.pickle', 'rb')\n",
    "data = pickle.load(pick)\n",
    "\n",
    "pick.close()\n",
    "\n",
    "#np.random(data)\n",
    "\n",
    "feature = []\n",
    "labels = []\n",
    "\n",
    "for img, label in data:\n",
    "\tfeature.append(img)\n",
    "\tlabels.append(label)\n",
    "\n",
    "#feature = np.array(feature, dtype = np.float32)\n",
    "#feature = feature/ 255.\n",
    "feature = np.array(feature)\n",
    "labels = np.array(labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "img_height = 350\n",
    "img_width = 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84430 files belonging to 10 classes.\n",
      "Using 67544 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84430 files belonging to 10 classes.\n",
      "Using 16886 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,1)\n",
    "    #plt.imshow(feature[i])\n",
    "    #plt.xlabel(categories[y_train[i]])\n",
    "    plt.xticks([])\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''x_train, x_test, y_train, y_test = train_test_split(feature, labels, test_size=0.1, shuffle=True)\n",
    "\n",
    "x_train, x_test = tf.cast(x_train, tf.float32), tf.cast(x_test, tf.float32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size = 10)\n",
    "'''\n",
    " \n",
    "\n",
    "model = VGGModel(classes = 10)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adadelta()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UMER\\.conda\\envs\\fyp\\lib\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16886/16886 [==============================] - 4773s 282ms/step - loss: 7.7888 - accuracy: 0.1057 - val_loss: 2.2840 - val_accuracy: 0.1086\n",
      "Epoch 2/2\n",
      "16886/16886 [==============================] - 4635s 274ms/step - loss: 2.2836 - accuracy: 0.1043 - val_loss: 2.2821 - val_accuracy: 0.0997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216ccffba60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training = True)\n",
    "        loss = loss_object(y_true=labels, y_pred=predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variable))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "for epoch in range(5):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for images, labels in train_dataset:\n",
    "        step+=1 \n",
    "        train_step(images, labels)\n",
    "        \n",
    "        if step%10==0:\n",
    "            \n",
    "            print('=> epoch: %i, loss: %.4f, train_accuracy: $.4f'%(epoch+1, train_loss.result(), train_accuracy.result()))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(x_test[0:9])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.xlabel('Predicted: %s\\n Actual: %s'%(categories[np.argmax(prediction[i])], categories[y_test[i]]))\n",
    "    \n",
    "    plt.xticks([])\n",
    "    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir('dataset frames')\n",
    "\n",
    "shot_types = os.listdir('dataset frames')\n",
    "print (shot_types)\n",
    "\n",
    "print(\"Types of shots: \", len(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = []\n",
    "\n",
    "for item in shot_types:\n",
    "    # Getting all the file names\n",
    "    all_shots = os.listdir('dataset frames' + '/' +item)\n",
    "\n",
    "\n",
    "    for shot in all_shots:\n",
    "        shots.append((item, str('dataset frames' + '/' +item) + '/' + shot))\n",
    "        #print(shots)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a dafaframe\n",
    "shots_df = pd.DataFrame(data=shots, columns=['Shot Type', 'Image'])\n",
    "print(shots_df.head)\n",
    "print(shots_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheking how many images are present for each shot\n",
    "print(\"Total number shot images in dataset: \", len(shots_df))\n",
    "\n",
    "shot_count = shots_df['Shot Type'].value_counts()\n",
    "\n",
    "print(\"Number of different shots: \")\n",
    "print(shot_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "d=100\n",
    "x=460\n",
    "h=350\n",
    "w=330\n",
    "img = cv2.imread(\"dataset frames\\\\cover_frames\\\\frame1_0.jpeg\")\n",
    "#plt.imshow(img)\n",
    "#crop_img = img[d:d+h, x:x+w]\n",
    "#cv2.imshow(\"cropped\", crop_img)\n",
    "plt.imshow(img)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = 'dataset frames/'\n",
    "\n",
    "#im_size = 180\n",
    "d=100\n",
    "x=460\n",
    "h=350\n",
    "w=330\n",
    "#images = np.array([])\n",
    "#labels = []\n",
    "count=0\n",
    "#for i in shot_types:\n",
    "data_path = path + str(\"sweep_frames\")\n",
    "filenames = [i for i in os.listdir(data_path)]\n",
    "\n",
    "for f in filenames:\n",
    "    print(f)\n",
    "    img = cv2.imread(data_path + '/' + f)\n",
    "    #plt.imshow(img)\n",
    "    #print(img.shape)\n",
    "    if(img.shape[0]<=350): continue\n",
    "    cimg = img[100:100+350, 460:460+330]\n",
    "    \n",
    "    cv2.imwrite(data_path + '/' + f,cimg)\n",
    "    #images = np.append(images,[img])\n",
    "    #labels.append(i)\n",
    "    count=count+1\n",
    "    #print(len(images),count)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "#np.save(\"csv\",images)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_white = np.array([0,0,168])\n",
    "lower_brown = np.array([10,100,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "\tpath = os.path.join(data_dir, category)\n",
    "\n",
    "    for img_name in os.listdir(path):\n",
    "        image_path = os.path.join(path, img_name)\n",
    "        cv2.imread(image_path)\n",
    "        #cv2.imshow(\"Original frame\", image_path)\n",
    "        hsv = cv2.cvtColor(image_path,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        mask = cv2.inRange(hsv, lower_white, lower_brown)\n",
    "        #cv2.imshow(\"Masked frame\", mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#np.save(\"csv\",images)\n",
    "#images = np.array(images)\n",
    "print(images)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "y = shots_df['Shot Type'].values\n",
    "\n",
    "y_labelencoder = LabelEncoder()\n",
    "y = y_labelencoder.fit_transform(y)\n",
    "print(y)\n",
    "#y.shape()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = y.reshape(1,-1)\n",
    "#onehotencoder = OneHotEncoder(categorical_features=[0])\n",
    "#Y = onehotencoder.fit_transform(y)\n",
    "#P = ColumnTransformer([(\"Shot\", OneHotEncoder(), [0])])\n",
    "#Y = P.fit_transform(y)\n",
    "#print(len(images))\n",
    "#Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#images=np.load(\"csv.npy\")\n",
    "images = images.astype('float32') / 255.0\n",
    "image, y = shuffle(images, y, random_state=1)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, y, test_size=0.175, random_state=415)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model.fit(train_x, train_y, epochs = 50, batch_size = 10)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''preds = model.evaluate(test_x, test_y)\n",
    "print(\"Loss = \" + str(preds[0]))\n",
    "#print(\"Test Acuuracy = \" + str(preds[1]))'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a530ab95cce582bf70bc95a4027aee1a6d716964b40576cc9899df131cc74cb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
