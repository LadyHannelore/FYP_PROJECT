{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcHUzAdqOYSF",
        "outputId": "d09d881d-3b49-4edd-8c12-0e2861e1e4e9"
      },
      "outputs": [],
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive/')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmN76FQuv33"
      },
      "source": [
        "**Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIO2mXSoeY2i"
      },
      "outputs": [],
      "source": [
        "import os #Sindh\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "#import pafy\n",
        "#import youtube_dl\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        " \n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.layers import Dense, Dropout, Flatten, TimeDistributed\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "#from keras.layers.recurrent import LSTM\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.metrics import binary_accuracy\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, Conv3D, MaxPooling3D, Flatten, LSTM, concatenate\n",
        "from tensorflow.keras.layers import Reshape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cE0VCzu5Zvx"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "Activation functions used:\n",
        "\n",
        "* ReLU (Rectified Linear Unit): ReLU is a simple activation function that maps any negative input to 0 and any positive input to itself. Mathematically, it is defined as f(x) = max(0,x). ReLU is commonly used in deep learning because it is easy to compute and has been shown to be effective in preventing the vanishing gradient problem.\n",
        "\n",
        "* Softmax: Softmax is a activation function that is often used in the output layer of a neural network for multiclass classification problems. It maps the outputs to a probability distribution over the classes. Mathematically, it is defined as f(x_i) = e^(x_i)/sum(e^(x_j)) for i=1,2,...,n where n is the number of classes. The output of the softmax function is a vector of probabilities that sums up to 1.\n",
        "\n",
        "* Tanh (Hyperbolic tangent): Tanh is a activation function that maps the input to a value between -1 and 1. It is defined as f(x) = (e^x - e^(-x))/(e^x + e^(-x)). Tanh is similar to the sigmoid function, but it is symmetric around the origin and has a steeper gradient near the origin. Tanh is often used in the hidden layers of neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v4Ft59V5Zvy"
      },
      "source": [
        "**Defining Seed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD4DCKnEfgxv"
      },
      "outputs": [],
      "source": [
        "seed_constant = 27 #Sindh\n",
        "np.random.seed(seed_constant)\n",
        "random.seed(seed_constant)\n",
        "tf.random.set_seed(seed_constant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0flzGJbDvDbV"
      },
      "source": [
        "**Loading Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8_IAimb8SSX"
      },
      "outputs": [],
      "source": [
        "dataset = './CricShot10 dataset/' #Sindh(Dataset path change karna pc pai jo ayega)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubb9tZJkvLz2"
      },
      "source": [
        "**Preprocessing Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "KL7d1O1vf0TP",
        "outputId": "f3903a8f-0c1e-4917-e792-10091a9aea64"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20, 20))  #Sindh\n",
        " \n",
        "#Get the names of all shots/categories in dataset.\n",
        "all_classes_names = ['cover', 'defense' , 'flick','hook','late_cut','lofted','pull','square_cut','straight','sweep']\n",
        "#all_classes_names = ['cover', 'defense' , 'flick']\n",
        "\n",
        " \n",
        "for classes in all_classes_names:\n",
        " \n",
        "    #selected_class_Name = classes\n",
        " \n",
        "    # Retrieving the list of all the video files present in shot Directories.\n",
        "    video_files_names_list = os.listdir(f'{dataset}/{classes}')\n",
        " \n",
        "    # Randomly selecting a video file from the list retrieved video files names list.\n",
        "    selected_video_file_name = random.choice(video_files_names_list)\n",
        " \n",
        "    video_reader = cv2.VideoCapture(f'{dataset}/{classes}/{selected_video_file_name}')\n",
        "    \n",
        "    _, bgr_frame = video_reader.read()\n",
        " \n",
        "    video_reader.release()\n",
        " \n",
        "    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
        "    cv2.putText(rgb_frame, classes, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "    plt.subplot(5, 4, 4);plt.imshow(rgb_frame);plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gleLD1regDLh"
      },
      "outputs": [],
      "source": [
        "# Specifying the height and width to which each video frame will be resized in our dataset.  #Sindh\n",
        "IMAGE_HEIGHT , IMAGE_WIDTH = 180, 180  \n",
        "\n",
        "# Specifying the number of frames of a video that will be fed to the model as one sequence.\n",
        "SEQUENCE_LENGTH = 10\n",
        "\n",
        "# Specifying the directory containing the dataset. \n",
        "DATASET_DIR = dataset\n",
        "\n",
        "# Specifying the list containing the names of the classes used for training.\n",
        "CLASSES_LIST = ['cover', 'defense' , 'flick']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHd8FK_8vSWm"
      },
      "source": [
        "**Extracting frames from videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0bN84RwitWz"
      },
      "outputs": [],
      "source": [
        "def frames_extraction(video_path):  #Sindh\n",
        "    '''\n",
        "    This function will extract the required frames from a video after resizing and normalizing them.\n",
        "    Args:\n",
        "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
        "    Returns:\n",
        "        frames_list: A list containing the resized and normalized frames of the video.\n",
        "    '''\n",
        "\n",
        "    frames_list = []\n",
        "    \n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculating the the interval after which frames will be added to the list.\n",
        "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
        "\n",
        "    for frame_counter in range(SEQUENCE_LENGTH):\n",
        "\n",
        "        # Setting the current frame position of the video.\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "\n",
        "        # Reading the frame from the video. \n",
        "        success, frame = video_reader.read() \n",
        "\n",
        "        # Checking if Video frame is not successfully read then break the loop\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        \n",
        "        # Normalizing the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "        normalized_frame = resized_frame / 255.0\n",
        "        \n",
        "        frames_list.append(normalized_frame)\n",
        "    \n",
        "    video_reader.release()\n",
        "\n",
        "    return frames_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9tS2tgYvYGd"
      },
      "source": [
        "**Dataset Creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvB3YKSNiyZf"
      },
      "outputs": [],
      "source": [
        "def create_dataset():  #Sindh\n",
        "    '''\n",
        "    This function will extract the data of the selected classes and create the required dataset.\n",
        "    Returns:\n",
        "        features:          A list containing the extracted frames of the videos.\n",
        "        labels:            A list containing the indexes of the classes associated with the videos.\n",
        "        video_files_paths: A list containing the paths of the videos in the disk.\n",
        "    '''\n",
        "\n",
        "    # Declared Empty Lists to store the features, labels and video file path values.\n",
        "    features = []\n",
        "    labels = []\n",
        "    video_files_paths = []\n",
        "    \n",
        "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
        "        \n",
        "        print(f'Extracting Data of Class: {class_name}')\n",
        "        \n",
        "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
        "        \n",
        "        for file_name in files_list:\n",
        "            \n",
        "            # Getting the complete video path.\n",
        "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
        "\n",
        "            # Extracting the frames of the video file.\n",
        "            frames = frames_extraction(video_file_path)\n",
        "\n",
        "            # Checking if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
        "            # So ignore the videos having frames less than the SEQUENCE_LENGTH.\n",
        "            if len(frames) == SEQUENCE_LENGTH:\n",
        "\n",
        "                # Append the data to their repective lists.\n",
        "                features.append(frames)\n",
        "                labels.append(class_index)\n",
        "                video_files_paths.append(video_file_path)\n",
        "\n",
        "    # Converting the list to numpy arrays\n",
        "    features = np.asarray(features)\n",
        "    labels = np.array(labels)  \n",
        "    \n",
        "    return features, labels, video_files_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7E6aAxwveFX"
      },
      "source": [
        "**Loading Features and Labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ3SUgNqi5yo",
        "outputId": "7d687b33-3610-4671-ead1-4b44fa3d9d7d"
      },
      "outputs": [],
      "source": [
        "features, labels, video_files_paths = create_dataset()  #Sindh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOVgj1-4SJOu",
        "outputId": "8e4f3eaf-4440-4c33-eda6-70f5b7469a81"
      },
      "outputs": [],
      "source": [
        "mean_image = np.mean(features)\n",
        "print(mean_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWa00soLi9UT"
      },
      "outputs": [],
      "source": [
        "# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\n",
        "one_hot_encoded_labels = to_categorical(labels)  #Sindh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqzS0YGtjAFh"
      },
      "outputs": [],
      "source": [
        "# Split the Data into Train ( 90% ) and Test Set ( 10% ).   #Sindh\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1, shuffle = True, random_state = seed_constant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ--BAE2vp4N"
      },
      "source": [
        "**Making custom LSTM model**\n",
        "\n",
        "The code defines a function called \"create_convlstm_model()\" that creates a Convolutional LSTM neural network model. The model has four ConvLSTM2D layers, each followed by a MaxPooling3D layer and a Dropout layer. The output of the last ConvLSTM2D layer is flattened and passed through a Dense layer with a softmax activation function. The model summary is printed and the function returns the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQMLaSaYjE2T"
      },
      "outputs": [],
      "source": [
        "def create_convlstm_model():   \n",
        "    '''\n",
        "    This function will construct the required convlstm model.\n",
        "    Returns:\n",
        "        model: It is the required constructed convlstm model.\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Defining the Model Architecture.\n",
        "    ########################################################################################################################\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,\n",
        "                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
        "                         recurrent_dropout=0.2, return_sequences=True))\n",
        "    \n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
        "    #model.add(TimeDistributed(Dropout(0.2)))\n",
        "    \n",
        "    model.add(Flatten()) \n",
        "    \n",
        "    model.add(Dense(len(CLASSES_LIST), activation = \"softmax\"))\n",
        "    \n",
        "    ########################################################################################################################\n",
        "     \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iY1NHzivwW1"
      },
      "source": [
        "**Model Structure/Shape**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9LBkVvZjVv0",
        "outputId": "fda913c6-e282-4338-9c37-95fb4ad4bb63"
      },
      "outputs": [],
      "source": [
        "convlstm_model = create_convlstm_model()\n",
        "\n",
        "print(\"Model Created Successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD3jHKYLjWlY"
      },
      "outputs": [],
      "source": [
        "# Ploting the structure of the contructed model.\n",
        "plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeZhwqD_v3cd"
      },
      "source": [
        "** *italicized text*Training the LSTM model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX3D4ZYn5Zv3"
      },
      "source": [
        "We will use early stopping because by stopping the training process early, before the model starts to overfit, we can achieve better generalization performance on new data. This is because the model is prevented from becoming too complex and fitting to the noise in the training data.\n",
        "Also, Cross-Entropy Loss will be used as it measures the difference between the predicted probability and the actual label for each training example. Optimizer \"Adam\" will be used as it has several advantages over gradient descent, such as fast convergence rates, automatic adaptation to the learning rate, and robustness to noisy gradients and sparse data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-cxjgdFjZlB",
        "outputId": "16976454-84c7-49ec-920f-fc4d168117ee"
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)\n",
        " \n",
        "convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
        "\n",
        "# Start training the model.\n",
        "convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 16,shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBNv18i0v_Ql"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewxBO68gjgRJ",
        "outputId": "be97a44c-d63a-4751-979f-b1b6730b6da3"
      },
      "outputs": [],
      "source": [
        "model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)\n",
        "#print('Test set accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwBiIKbCjpMH"
      },
      "outputs": [],
      "source": [
        "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
        "\n",
        "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
        "current_date_time_dt = dt.datetime.now()\n",
        "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
        " \n",
        "# Defining a useful name for our model to make it easy for us while navigating through multiple saved models.\n",
        "model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
        " \n",
        "convlstm_model.save(model_file_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEjDoqd6wH6U"
      },
      "source": [
        "**Plotting Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ona7bG6UjqEV"
      },
      "outputs": [],
      "source": [
        "def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n",
        "    '''\n",
        "    This function will plot the metrics passed to it in a graph.\n",
        "    Args:\n",
        "        model_training_history: A history object containing a record of training and validation \n",
        "                                loss values and metrics values at successive epochs\n",
        "        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n",
        "        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n",
        "        plot_name:              The title of the graph.\n",
        "    '''\n",
        "    \n",
        "    metric_value_1 = model_training_history.history[metric_name_1]\n",
        "    metric_value_2 = model_training_history.history[metric_name_2]\n",
        "    \n",
        "    # Constructing a range object which will be used as x-axis (horizontal plane) of the graph.\n",
        "    epochs = range(len(metric_value_1))\n",
        " \n",
        "    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
        "    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n",
        " \n",
        "    plt.title(str(plot_name))\n",
        " \n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZAT_cC_5Zv4"
      },
      "source": [
        "**Visualizing the training and validation loss metrices.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvLx0e5SjuPO"
      },
      "outputs": [],
      "source": [
        "plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_XaoHyJ5Zv4"
      },
      "source": [
        "**Visualizing the training and validation accuracy metrices.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSew7cfXjwph"
      },
      "outputs": [],
      "source": [
        "plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7GeG13PwM_8"
      },
      "source": [
        "**Making custom LRCN model**\n",
        "\n",
        "The given code creates a function \"create_LRCN_model\" that returns a Long-term Recurrent Convolutional Networks (LRCN) model for performing action recognition in videos. The model consists of several layers including Conv2D, MaxPooling2D, Dropout, and LSTM layers. The model takes input in the form of sequences of video frames, each of which is processed by the TimeDistributed layer, followed by LSTM and Dense layers for classification.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flKTdQ4Ujywt"
      },
      "outputs": [],
      "source": [
        "def create_LRCN_model():\n",
        "    '''\n",
        "    This function will construct the required LRCN model.\n",
        "    Returns:\n",
        "        model: It is the required constructed LRCN model.\n",
        "    '''\n",
        " \n",
        "    model = Sequential()\n",
        "    \n",
        "    # Defining the Model Architecture.\n",
        "    ########################################################################################################################\n",
        "    \n",
        "    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),\n",
        "                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
        "    \n",
        "    model.add(TimeDistributed(MaxPooling2D((4, 4)))) \n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "    \n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "    \n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    model.add(TimeDistributed(Dropout(0.25)))\n",
        "    \n",
        "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
        "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "    #model.add(TimeDistributed(Dropout(0.25)))\n",
        "                                      \n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "                                      \n",
        "    model.add(LSTM(32))\n",
        "                                      \n",
        "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
        " \n",
        "    ########################################################################################################################\n",
        " \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8QLgnBw5Zv4"
      },
      "source": [
        "**Construct the required LRCN model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sklKu_gj3lX",
        "outputId": "e824d1c1-d4b5-4963-fcde-64d0485ed01a"
      },
      "outputs": [],
      "source": [
        "LRCN_model = create_LRCN_model()\n",
        " \n",
        "print(\"Model Created Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8flNwokg5Zv5"
      },
      "source": [
        "**Plotting the structure of the contructed LRCN model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMvcgHDcj6Vz"
      },
      "outputs": [],
      "source": [
        "plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siauDd_p5Zv5"
      },
      "source": [
        "**Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iJf5koAj94p",
        "outputId": "5846cbcd-0943-4808-8a5f-97d8220b71b0"
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        " \n",
        "LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
        " \n",
        "LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 70, batch_size = 16 , shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txK3xjzj5Zv5"
      },
      "source": [
        "**Evaluating the trained model.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP1Qd3GJkCNP",
        "outputId": "10fff01f-2621-472b-cf33-84aa91e0997c"
      },
      "outputs": [],
      "source": [
        "model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)\n",
        "#print('Test set accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jouDcdy5Zv5"
      },
      "source": [
        "**Getting the loss and accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBZV3mMFkEth"
      },
      "outputs": [],
      "source": [
        "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
        " \n",
        "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
        "current_date_time_dt = dt.datetime.now()\n",
        "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
        "    \n",
        "model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
        " \n",
        "LRCN_model.save(model_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWOYRZvK5Zv6"
      },
      "source": [
        "**Visualizing the training and validation loss metrices.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwC6wX7PkKsg"
      },
      "outputs": [],
      "source": [
        "plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn0YDdnu5Zv6"
      },
      "source": [
        "**Visualizing the training and validation accuracy metrices.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTy6qevQkNTY"
      },
      "outputs": [],
      "source": [
        "plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPn58irQ5Zv6"
      },
      "source": [
        "The following code is a Python function that performs single action recognition prediction on a video using a LRCN model. It takes the path of the video file and a fixed number of frames as input. The function first reads the video file and extracts frames from it, resizes them, and normalizes them by dividing their values by 255. These preprocessed frames are then passed to the LRCN model to get the predicted probabilities for each action. The function selects the action with the highest probability and prints its name along with the confidence. Finally, the function releases the video reader object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p60FRXubLoIa"
      },
      "outputs": [],
      "source": [
        "def predict_single_action_LRCN(video_file_path, SEQUENCE_LENGTH):\n",
        "    '''\n",
        "    This function will perform single action recognition prediction on a video using the LRCN model.\n",
        "    Args:\n",
        "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
        "    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
        "    '''\n",
        " \n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        " \n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        " \n",
        "    frames_list = []\n",
        "    \n",
        "    predicted_class_name = ''\n",
        " \n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        " \n",
        "    # Calculate the interval after which frames will be added to the list.\n",
        "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n",
        " \n",
        "    for frame_counter in range(SEQUENCE_LENGTH):\n",
        " \n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        " \n",
        "        success, frame = video_reader.read() \n",
        " \n",
        "        # Check if frame is not read properly then break the loop.\n",
        "        if not success:\n",
        "            break\n",
        " \n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        \n",
        "        normalized_frame = resized_frame / 255\n",
        "        \n",
        "        frames_list.append(normalized_frame)\n",
        " \n",
        "    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n",
        "    predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_list, axis = 0))[0]\n",
        " \n",
        "    predicted_label = np.argmax(predicted_labels_probabilities)\n",
        " \n",
        "    predicted_class_name = CLASSES_LIST[predicted_label]\n",
        "    \n",
        "    print(f'Action Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n",
        "        \n",
        "    video_reader.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLZ9hdhj5Zv6"
      },
      "source": [
        "**Input video file paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ENvgT6fLnhg"
      },
      "outputs": [],
      "source": [
        "input_video_path = '/content/drive/MyDrive/cover video.avi'\n",
        "input_video_path_2 = '/content/drive/MyDrive/defense video.avi'\n",
        "input_video_path_3 = '/content/drive/MyDrive/flick video.avi'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRoZ8i555Zv7"
      },
      "source": [
        "**Perform Single Prediction on the Test Videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOu28fg-LsZI"
      },
      "outputs": [],
      "source": [
        "predict_single_action_LRCN(input_video_path, SEQUENCE_LENGTH)\n",
        "predict_single_action_LRCN(input_video_path_2, SEQUENCE_LENGTH)\n",
        "predict_single_action_LRCN(input_video_path_3, SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faRmjY-g5Zv7"
      },
      "source": [
        "The following code defines a function that creates a 3D convolutional neural network model for action recognition. The model takes as input a sequence of images, with a fixed length, and has several convolutional and max pooling layers. The output of the convolutional layers is flattened and passed through a dense layer with a softmax activation function, which predicts the probability of each action class. The model is returned by the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ossD3_DAb2X4"
      },
      "outputs": [],
      "source": [
        "def create_3D_CNN():   #Sindh\n",
        "  '''\n",
        "    This function will construct the required 3DCNN model.\n",
        "    Returns:\n",
        "        model: It is the required constructed 3DCNN model.\n",
        "  '''\n",
        "\n",
        "  # Defining the Model Architecture.\n",
        "    ########################################################################################################################\n",
        "\n",
        "  input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
        "\n",
        "  inputs = Input(shape=input_shape)\n",
        "\n",
        "  x = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(inputs)\n",
        "  \n",
        "  x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "  \n",
        "  x = (TimeDistributed(Dropout(0.2)))(x)\n",
        "  \n",
        "  x = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(x)\n",
        "  \n",
        "  x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  outputs = Dense(units=len(CLASSES_LIST), activation='softmax')(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  ########################################################################################################################\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiiCRA_15Zv7"
      },
      "source": [
        "**Construct the required 3D CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlcO8aicseVC",
        "outputId": "4958f089-b5e6-4757-d0f5-b777d6c59a1f"
      },
      "outputs": [],
      "source": [
        "CNN3D_model = create_3D_CNN()   #Sindh\n",
        "print(\"Model Created Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNjTizfm5Zv8"
      },
      "source": [
        "**Plotting the structure of the contructed 3D CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "xaCujuLfKQR8",
        "outputId": "5fabb006-4018-4df0-c27c-25ec87238566"
      },
      "outputs": [],
      "source": [
        "plot_model(CNN3D_model, to_file = 'CNN3D_model_structure_plot.png', show_shapes = True, show_layer_names = True)   #Sindh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iwfmpne5Zv8"
      },
      "source": [
        "**Start training the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkhwBILxLDIS"
      },
      "outputs": [],
      "source": [
        "CNN3D_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])   #Sindh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9aLM1q0IQ7W"
      },
      "outputs": [],
      "source": [
        "# Define the number of folds for cross-validation\n",
        "k = 5            #Sindh\n",
        "\n",
        "# Load your 3D data here\n",
        "X = features_train\n",
        "y = labels_train\n",
        "\n",
        "# Define the K-fold cross-validator\n",
        "kf = KFold(n_splits=k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zskcm9smJJ9u",
        "outputId": "1a8923aa-e6d8-455b-a094-132e8bfb6ed6"
      },
      "outputs": [],
      "source": [
        "# Iterate over each fold                  #Sindh\n",
        "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
        "    \n",
        "    print(f'Fold {i+1}/{k}')\n",
        "    \n",
        "    # Split the data into training and validation sets for this fold\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "    \n",
        "    # Fit the model on the training set for this fold\n",
        "    CNN3D_model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val))\n",
        "    \n",
        "    # Evaluate the model on the validation set for this fold\n",
        "    scores = CNN3D_model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f'Validation accuracy: {scores[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crXhwmbEtjQb",
        "outputId": "ee07c605-73c7-4f4f-c411-2d7075f66866"
      },
      "outputs": [],
      "source": [
        "model_evaluation_history = CNN3D_model.evaluate(features_test, labels_test)\n",
        "#print('Test set accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoIonR3etIix"
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        " \n",
        "CNN3D_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
        " \n",
        "CNN3D_model_training_history = CNN3D_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 32 , shuffle = True, validation_split = 0.1, callbacks = [early_stopping_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRMEbTth5Zv8"
      },
      "source": [
        "**Evaluating the trained model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEjciHk5Zv9"
      },
      "source": [
        "**Visualizing the training and validation loss metrices.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "a_xHEQr25zu4",
        "outputId": "201d311d-d7cc-439d-cb17-949ea4de7a9f"
      },
      "outputs": [],
      "source": [
        "plot_metric(CNN3D_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_4lLknY5Zv9"
      },
      "source": [
        "**Visualizing the training and validation accuracy metrices.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "nZp3hQld542I",
        "outputId": "e7253434-8508-46b5-89a4-bb9a63ab979a"
      },
      "outputs": [],
      "source": [
        "plot_metric(CNN3D_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny7x3STx5Zv9"
      },
      "source": [
        "The following code defines a function called \"predict_single_action_CNN3D\", which predicts a single action recognition for a given video file using a pre-trained CNN3D model. The function first initializes a VideoCapture object to read the frames from the video file. It then extracts a fixed number of frames (SEQUENCE_LENGTH) from the video with a fixed skip interval, and preprocesses each frame by resizing and normalizing it. The preprocessed frames are then passed to the pre-trained CNN3D model to obtain predicted probabilities. The function prints the predicted action and its confidence score, and releases the VideoCapture object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9BqUPDzNqF9"
      },
      "outputs": [],
      "source": [
        "def predict_single_action_CNN3D(video_file_path, SEQUENCE_LENGTH):\n",
        "    '''\n",
        "    This function will perform single action recognition prediction on a video using the 3D CNN model.\n",
        "    Args:\n",
        "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
        "    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
        "    '''\n",
        " \n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        " \n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        " \n",
        "    frames_list = []\n",
        "    \n",
        "    predicted_class_name = ''\n",
        " \n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        " \n",
        "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n",
        " \n",
        "    for frame_counter in range(SEQUENCE_LENGTH):\n",
        " \n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        " \n",
        "        success, frame = video_reader.read() \n",
        " \n",
        "        # Check if frame is not read properly then break the loop.\n",
        "        if not success:\n",
        "            break\n",
        " \n",
        "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "        \n",
        "        normalized_frame = resized_frame / 255\n",
        "        \n",
        "        frames_list.append(normalized_frame)\n",
        " \n",
        "    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n",
        "    predicted_labels_probabilities = CNN3D_model.predict(np.expand_dims(frames_list, axis = 0))[0]\n",
        " \n",
        "    # Get the index of class with highest probability.\n",
        "    predicted_label = np.argmax(predicted_labels_probabilities)\n",
        " \n",
        "    predicted_class_name = CLASSES_LIST[predicted_label]\n",
        "    \n",
        "    print(f'Action Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n",
        "        \n",
        "    video_reader.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10p7rvpF5Zv-"
      },
      "source": [
        "**Input video paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKDEXZ8qOC2G"
      },
      "outputs": [],
      "source": [
        "input_video_path = '/content/drive/MyDrive/Val data/Video 1.avi'  #cover\n",
        "input_video_path_2 = '/content/drive/MyDrive/Val data/Video 3.avi'  #defence\n",
        "input_video_path_3 = '/content/drive/MyDrive/Val data/Video 6.avi'  #flick\n",
        "input_video_path_4 = '/content/drive/MyDrive/Val data/Video 2.avi'  #cover\n",
        "input_video_path_5 = '/content/drive/MyDrive/Val data/Video 5.avi'  #flick\n",
        "input_video_path_6 = '/content/drive/MyDrive/Val data/Video 4.avi'  #defence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSBU9IS45Zv-"
      },
      "source": [
        "**Perform Single Prediction on the Test Videos**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvF09flwN2HV",
        "outputId": "819783b6-ddb7-4813-e215-30b17c878b36"
      },
      "outputs": [],
      "source": [
        "predict_single_action_CNN3D(input_video_path, SEQUENCE_LENGTH)\n",
        "predict_single_action_CNN3D(input_video_path_2, SEQUENCE_LENGTH)\n",
        "predict_single_action_CNN3D(input_video_path_3, SEQUENCE_LENGTH)\n",
        "predict_single_action_CNN3D(input_video_path_4, SEQUENCE_LENGTH)\n",
        "predict_single_action_CNN3D(input_video_path_5, SEQUENCE_LENGTH)\n",
        "predict_single_action_CNN3D(input_video_path_6, SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Define the input shape of the model\n",
        "input_shape = (10, 180, 180, 3)   # 10 frames of size 180x180 with 3 color channels (RGB)\n",
        "\n",
        "# Open a video capture device for real-time video input\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Define a loop to read frames from the video stream and classify them\n",
        "frames = []  # list to hold the sequence of frames\n",
        "while True:\n",
        "    # Read a frame from the video stream\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Resize the frame to match the input shape of the model\n",
        "    frame = cv2.resize(frame, (input_shape[2], input_shape[1]))\n",
        "\n",
        "    # Add the frame to the list of frames\n",
        "    frames.append(frame)\n",
        "\n",
        "    # If the list of frames has the desired sequence length, preprocess and feed them into the model\n",
        "    if len(frames) == input_shape[0]:\n",
        "        # Convert the list of frames to a numpy array with shape (1, 10, 180, 180, 3)\n",
        "        frame_sequence = np.array([frames], dtype=np.float32) / 255.0\n",
        "        # Predict the class probabilities for the input frame sequence\n",
        "        probabilities = CNN3D_model.predict(frame_sequence)\n",
        "        # Get the predicted class label\n",
        "        predicted_class = np.argmax(probabilities)\n",
        "        # Display the predicted class label on the frame\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        font_scale = 1\n",
        "        thickness = 2\n",
        "        text = 'Class: {}'.format(predicted_class)\n",
        "        text_size, _ = cv2.getTextSize(text, font, font_scale, thickness)\n",
        "        text_x = 10\n",
        "        text_y = 10 + text_size[1]\n",
        "        cv2.putText(frame, text, (text_x, text_y), font, font_scale, (0, 255, 0), thickness)\n",
        "        # Define the size of the window\n",
        "        '''WINDOW_WIDTH = 400\n",
        "        WINDOW_HEIGHT = 200\n",
        "\n",
        "        # Create a named window with the desired size\n",
        "        cv2.namedWindow('Real-time video classification', cv2.WINDOW_NORMAL)\n",
        "        cv2.resizeWindow('Real-time video classification', WINDOW_WIDTH, WINDOW_HEIGHT)\n",
        "'''\n",
        "        # Display the frame\n",
        "        cv2.imshow('Real-time video classification', frame)\n",
        "\n",
        "        # Clear the list of frames to start a new sequence\n",
        "        frames.clear()\n",
        "\n",
        "    # Wait for a key press\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture device and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "fyp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "a530ab95cce582bf70bc95a4027aee1a6d716964b40576cc9899df131cc74cb7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
